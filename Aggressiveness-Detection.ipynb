{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers as ppb\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertForMaskedLM, BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def to_int(x):\n",
    "    return list(map(int,x))\n",
    "\n",
    "def to_low(x):\n",
    "    return [w.lower() for w in x]\n",
    "\n",
    "def test_model(model, test_batches, _config):\n",
    "    device = torch.device(_config['device'])\n",
    "    y_t=[]\n",
    "    y_p=[]\n",
    "    logits = []\n",
    "    for sample in test_batches:\n",
    "        x, y, att = sample['text'].to(device), sample['label'].to(device), sample['attention'].to(device)\n",
    "        y_pred = F.softmax(model(x, att).cpu().detach(),1)\n",
    "        logits.append(y_pred)\n",
    "        y_pred = y_pred.argmax(1)\n",
    "        y_p.append(y_pred)\n",
    "        y_t.append(y.cpu())\n",
    "    logits = torch.cat(logits)\n",
    "    y_p=torch.cat(y_p)\n",
    "    y_t=torch.cat(y_t)\n",
    "    return f1_score(y_t,y_p,average='binary')\n",
    "\n",
    "def get_logits(model, test_batches, _config):\n",
    "    device = torch.device(_config['device'])\n",
    "    logits = []\n",
    "    for sample in test_batches:\n",
    "        x, att = sample['text'].to(device), sample['attention'].to(device)\n",
    "        y_pred = F.softmax(model(x, att).cpu().detach(),1)\n",
    "        logits.append(y_pred)\n",
    "    logits = torch.cat(logits)   \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader\n",
    "class MEXA3T(Dataset):\n",
    "    def __init__(self, X, y = None, _config = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(_config['model_name'], do_lower_case=True)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        out=torch.tensor(self.tokenizer.encode(self.X[idx], max_length=64, pad_to_max_length=True, add_special_tokens=True, truncation_strategy = 'longest_first'))\n",
    "        if(self.y):\n",
    "            return {\"text\": out, \"attention\":(out!=1).float(), \"label\":torch.tensor(self.y[idx])}\n",
    "        else:\n",
    "            return {\"text\": out, \"attention\":(out!=1).float()}\n",
    "\n",
    "# Define model\n",
    "class BERTForSequenceClassification(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BERTForSequenceClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.clf  = nn.Linear(768, 2, bias=True)\n",
    "    def forward(self, x, att):\n",
    "        x = self.bert(x, attention_mask = att)[1]\n",
    "        x = self.drop(x)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(_config, train, test, verbose = True):\n",
    "    device = torch.device(_config['device'])\n",
    "    w = _config['weights']\n",
    "    lr = _config['lr']\n",
    "    max_grad_norm = 1.0\n",
    "    epochs = _config['epochs']\n",
    "    for k in range(0, _config['n_models']):\n",
    "        train_batches = DataLoader(train, batch_size = _config['train_batch_size'], shuffle = True)\n",
    "        test_batches = DataLoader(test, batch_size = _config['test_batch_size'], shuffle = False)\n",
    "        \n",
    "        model = BERTForSequenceClassification(_config['model_name']).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "        total_steps = len(train_batches) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "        criterio = nn.CrossEntropyLoss(weight = w.to(device))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for sample in train_batches:\n",
    "                optimizer.zero_grad()\n",
    "                x, y, att = sample['text'].to(device), sample['label'].to(device), sample['attention'].to(device)\n",
    "                y_pred = model(x, att)\n",
    "                loss = criterio(y_pred, y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "        if(verbose):\n",
    "            model.eval()\n",
    "            tmp_m = test_model(model, test_batches, _config)\n",
    "            print(\"Model %d \\t f_1 = %.4f\"%(k, tmp_m*100))\n",
    "        torch.save(model.state_dict(), 'Models/model_'+str(k)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/train.csv')\n",
    "df_val  = pd.read_csv('Data/val.csv')\n",
    "df_test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# Prepare train\n",
    "X_train = to_low(list(df_train['text']))+to_low(list(df_val['text']))\n",
    "y_train = to_int(list(df_train['target']))+to_int(list(df_val['target']))\n",
    "\n",
    "# Prepare test\n",
    "X_test = to_low(list(df_test['text']))\n",
    "y_test = to_int(list(df_test['target']))\n",
    "\n",
    "w = (lambda a, b: torch.tensor([max(a, b)/a, max(a,b)/b]))((torch.tensor(y_train)==0).sum().float(), (torch.tensor(y_train)==1).sum().float())\n",
    "\n",
    "_config = {\n",
    "    'model_name': 'dccuchile/bert-base-spanish-wwm-uncased',\n",
    "    'train_batch_size':  32,\n",
    "    'test_batch_size': 128,\n",
    "    'device': 'cuda:1',\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 3,\n",
    "    'n_models': 3,\n",
    "    'weights': w\n",
    "}\n",
    "\n",
    "train = MEXA3T(X_train, y_train, _config) \n",
    "test = MEXA3T(X_test, y_test, _config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 \t f_1 = 78.5185\n",
      "Model 1 \t f_1 = 79.6186\n",
      "Model 2 \t f_1 = 79.0865\n"
     ]
    }
   ],
   "source": [
    "_config['n_models'] = 3\n",
    "train_models(_config, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = DataLoader(test, batch_size = _config['test_batch_size'], shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(_config['device'])\n",
    "model = BERTForSequenceClassification(_config['model_name']).to(device)\n",
    "x = []\n",
    "for i in tqdm.tqdm(range(_config['n_models'])):\n",
    "    model.load_state_dict(torch.load(\"Models/model_\"+str(i)+\".pt\"))\n",
    "    model.eval()\n",
    "    x.append(get_logits(model, test_batches, _config).unsqueeze(1))\n",
    "    \n",
    "X = torch.cat(x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.75903614457832"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority Voting \n",
    "y_pred = (X.argmax(2).sum(1).float()/_config['n_models']).round()\n",
    "f1_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.28846153846153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighted Voting \n",
    "y_pred= X.sum(1).argmax(1)\n",
    "f1_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
